{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-10-18T10:17:19.972912Z",
     "iopub.status.busy": "2024-10-18T10:17:19.972600Z",
     "iopub.status.idle": "2024-10-18T10:17:34.369766Z",
     "shell.execute_reply": "2024-10-18T10:17:34.368645Z",
     "shell.execute_reply.started": "2024-10-18T10:17:19.972877Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting jiwer\n",
      "  Downloading jiwer-3.0.4-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: click<9.0.0,>=8.1.3 in /opt/conda/lib/python3.10/site-packages (from jiwer) (8.1.7)\n",
      "Collecting rapidfuzz<4,>=3 (from jiwer)\n",
      "  Downloading rapidfuzz-3.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Downloading jiwer-3.0.4-py3-none-any.whl (21 kB)\n",
      "Downloading rapidfuzz-3.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: rapidfuzz, jiwer\n",
      "Successfully installed jiwer-3.0.4 rapidfuzz-3.10.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install jiwer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-18T10:17:34.372224Z",
     "iopub.status.busy": "2024-10-18T10:17:34.371870Z",
     "iopub.status.idle": "2024-10-18T10:17:57.276874Z",
     "shell.execute_reply": "2024-10-18T10:17:57.275930Z",
     "shell.execute_reply.started": "2024-10-18T10:17:34.372184Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.45.1)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.45.2-py3-none-any.whl.metadata (44 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.15.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.25.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.5.15)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.20.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.8.30)\n",
      "Downloading transformers-4.45.2-py3-none-any.whl (9.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m62.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: transformers\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.45.1\n",
      "    Uninstalling transformers-4.45.1:\n",
      "      Successfully uninstalled transformers-4.45.1\n",
      "Successfully installed transformers-4.45.2\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-18T10:17:57.278602Z",
     "iopub.status.busy": "2024-10-18T10:17:57.278276Z",
     "iopub.status.idle": "2024-10-18T10:18:16.818743Z",
     "shell.execute_reply": "2024-10-18T10:18:16.817969Z",
     "shell.execute_reply.started": "2024-10-18T10:17:57.278567Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import VisionEncoderDecoderModel, AutoProcessor\n",
    "from datasets import load_dataset\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.notebook import tqdm_notebook as tqdm\n",
    "from transformers import AdamW\n",
    "import torch.nn.functional as F\n",
    "device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "from jiwer import wer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-18T10:18:16.821366Z",
     "iopub.status.busy": "2024-10-18T10:18:16.820743Z",
     "iopub.status.idle": "2024-10-18T10:18:47.735925Z",
     "shell.execute_reply": "2024-10-18T10:18:47.735109Z",
     "shell.execute_reply.started": "2024-10-18T10:18:16.821330Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48a1df55f0a943edbb809dc87cacb46b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/4.97k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e72c45f691f4484e8e082fd8ddfc3296",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/2.44G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VisionEncoderDecoderModel has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "784762c890d444dc8e3bd069d1da773a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/420 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0011ff65daf24c6a8113a6bc92b20249",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/364 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb3c467a3b8e4dab98c7644fbed8d067",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.38k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c20366d0bc8741b28381f13ba5db3cf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86e1f47265624a10ae31298daaad55d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbe061dacf6448689808bafe21bbafa5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8221dd3412e4927a7c7dadb73d49c70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/957 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the model and processor\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"qantev/trocr-large-spanish\")\n",
    "processor = AutoProcessor.from_pretrained(\"qantev/trocr-large-spanish\")\n",
    "tokenizer = TrOCRProcessor.from_pretrained('qantev/trocr-large-spanish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-18T10:18:47.737358Z",
     "iopub.status.busy": "2024-10-18T10:18:47.737052Z",
     "iopub.status.idle": "2024-10-18T10:19:05.890089Z",
     "shell.execute_reply": "2024-10-18T10:19:05.889192Z",
     "shell.execute_reply.started": "2024-10-18T10:18:47.737326Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_30/3069459000.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"/kaggle/input/large-trocr-8-epochs-post-process/model_weights_large_grayscale_contrast (1).pth\"))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"/kaggle/input/large-trocr-8-epochs-post-process/model_weights_large_grayscale_contrast (1).pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-18T10:19:05.891771Z",
     "iopub.status.busy": "2024-10-18T10:19:05.891454Z",
     "iopub.status.idle": "2024-10-18T10:19:05.906790Z",
     "shell.execute_reply": "2024-10-18T10:19:05.905898Z",
     "shell.execute_reply.started": "2024-10-18T10:19:05.891739Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('/kaggle/input/ai-of-god-3/Public_data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-18T10:19:05.908272Z",
     "iopub.status.busy": "2024-10-18T10:19:05.907942Z",
     "iopub.status.idle": "2024-10-18T10:19:05.927833Z",
     "shell.execute_reply": "2024-10-18T10:19:05.926871Z",
     "shell.execute_reply.started": "2024-10-18T10:19:05.908240Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          image_path\n",
      "0  /kaggle/input/ai-of-god-3/Public_data/test_ima...\n",
      "1  /kaggle/input/ai-of-god-3/Public_data/test_ima...\n",
      "2  /kaggle/input/ai-of-god-3/Public_data/test_ima...\n",
      "3  /kaggle/input/ai-of-god-3/Public_data/test_ima...\n",
      "4  /kaggle/input/ai-of-god-3/Public_data/test_ima...\n"
     ]
    }
   ],
   "source": [
    "test_path = '/kaggle/input/ai-of-god-3/Public_data/test_images'\n",
    "\n",
    "# Function to convert unique ID into correct image path\n",
    "def construct_image_path(unique_id):\n",
    "    parts = unique_id.split('_')\n",
    "    page_number = parts[1]\n",
    "    l_number = parts[3] \n",
    "    \n",
    "    image_path = os.path.join(test_path, f'Page_{page_number}', f'L_{l_number}.png')\n",
    "    return image_path\n",
    "\n",
    "test_df['image_path'] = test_df['unique Id'].apply(construct_image_path)\n",
    "\n",
    "# Check the resulting dataframe\n",
    "print(test_df[[ 'image_path']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-18T10:19:05.929413Z",
     "iopub.status.busy": "2024-10-18T10:19:05.929052Z",
     "iopub.status.idle": "2024-10-18T10:19:05.933989Z",
     "shell.execute_reply": "2024-10-18T10:19:05.933061Z",
     "shell.execute_reply.started": "2024-10-18T10:19:05.929376Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-18T10:19:05.935549Z",
     "iopub.status.busy": "2024-10-18T10:19:05.935225Z",
     "iopub.status.idle": "2024-10-18T10:19:06.621866Z",
     "shell.execute_reply": "2024-10-18T10:19:06.620894Z",
     "shell.execute_reply.started": "2024-10-18T10:19:05.935517Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VisionEncoderDecoderModel(\n",
       "  (encoder): ViTModel(\n",
       "    (embeddings): ViTEmbeddings(\n",
       "      (patch_embeddings): ViTPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): ViTEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-23): 24 x ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "    (pooler): ViTPooler(\n",
       "      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (decoder): TrOCRForCausalLM(\n",
       "    (model): TrOCRDecoderWrapper(\n",
       "      (decoder): TrOCRDecoder(\n",
       "        (embed_tokens): TrOCRScaledWordEmbedding(50265, 1024, padding_idx=1)\n",
       "        (embed_positions): TrOCRSinusoidalPositionalEmbedding()\n",
       "        (layers): ModuleList(\n",
       "          (0-11): 12 x TrOCRDecoderLayer(\n",
       "            (self_attn): TrOCRAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (activation_fn): ReLU()\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): TrOCRAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (output_projection): Linear(in_features=1024, out_features=50265, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-18T10:19:06.658277Z",
     "iopub.status.busy": "2024-10-18T10:19:06.657964Z",
     "iopub.status.idle": "2024-10-18T10:20:00.966125Z",
     "shell.execute_reply": "2024-10-18T10:20:00.965186Z",
     "shell.execute_reply.started": "2024-10-18T10:19:06.658245Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0088e948e374f458e2dc083e235a9fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inference:   0%|          | 0/168 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1338: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed in v5. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1220: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():  # No need to calculate gradients during inference\n",
    "    for _, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"Inference\"):\n",
    "        image_path = row['image_path']\n",
    "        \n",
    "        # Open the image as grayscale\n",
    "        image = Image.open(image_path).convert(\"L\")  # 'L' mode for grayscale\n",
    "        \n",
    "        # Transform the image to tensor format\n",
    "        transform = transforms.ToTensor()\n",
    "        image_tensor = transform(image)  # Shape: [1, H, W] for grayscale\n",
    "        \n",
    "        # Stack the grayscale image to 3 channels to match model input\n",
    "        image_stacked = torch.cat([image_tensor] * 3, dim=0)  # Shape: [3, H, W]\n",
    "\n",
    "        # Apply threshold transformation\n",
    "        mask = image_stacked < 0.6875\n",
    "        image_stacked[mask] = 0.0\n",
    "        image_stacked[~mask] = 1.0\n",
    "\n",
    "        # Reshape the image for batch processing\n",
    "        image_stacked = image_stacked.unsqueeze(0)  # Add batch dimension, Shape: [1, 3, H, W]\n",
    "\n",
    "        # Process the image using the processor without rescaling (as per your requirement)\n",
    "        pixel_values = processor(image_stacked, return_tensors=\"pt\", do_rescale=False).pixel_values.to(device)\n",
    "        \n",
    "        # Generate the output using the model\n",
    "        outputs = model.generate(pixel_values)\n",
    "\n",
    "        # Decode the generated output to get the predicted string\n",
    "        predicted_string = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Append the results to predictions list\n",
    "        predictions.append({\n",
    "            'unique Id': row['unique Id'],  # Ensure 'unique Id' matches your column name\n",
    "            'prediction': predicted_string\n",
    "        })\n",
    "\n",
    "# Create a DataFrame for storing predictions\n",
    "predictions_df = pd.DataFrame(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-18T10:20:00.968026Z",
     "iopub.status.busy": "2024-10-18T10:20:00.967550Z",
     "iopub.status.idle": "2024-10-18T10:20:00.976532Z",
     "shell.execute_reply": "2024-10-18T10:20:00.975600Z",
     "shell.execute_reply.started": "2024-10-18T10:20:00.967981Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def preprocess_transcription(text):\n",
    "    \"\"\"\n",
    "    Preprocess the transcriptions based on Old Spanish characteristics.\n",
    "    \"\"\"\n",
    "    # Handle interchangeable letters\n",
    "    text = re.sub(r'(?<!\\S)[uU](?!\\S)', 'v', text)  # u -> v\n",
    "    text = re.sub(r'(?<!\\S)[vV](?!\\S)', 'u', text)  # v -> u\n",
    "    text = re.sub(r'(?<!\\S)[fF](?!\\S)', 's', text)  # f -> s\n",
    "    text = re.sub(r'(?<!\\S)[sS](?!\\S)', 'f', text)  # s -> f\n",
    "    \n",
    "    # Remove accents\n",
    "    text = re.sub(r'[áàäâ]', 'a', text)\n",
    "    text = re.sub(r'[éèëê]', 'e', text)\n",
    "    text = re.sub(r'[íìïî]', 'i', text)\n",
    "    text = re.sub(r'[óòöô]', 'o', text)\n",
    "    text = re.sub(r'[úùüû]', 'u', text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-18T10:20:00.978054Z",
     "iopub.status.busy": "2024-10-18T10:20:00.977791Z",
     "iopub.status.idle": "2024-10-18T10:20:00.999494Z",
     "shell.execute_reply": "2024-10-18T10:20:00.998717Z",
     "shell.execute_reply.started": "2024-10-18T10:20:00.978025Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "predictions_df['postprocessed_transcription'] = predictions_df['prediction'].apply(preprocess_transcription)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-18T10:20:01.000804Z",
     "iopub.status.busy": "2024-10-18T10:20:01.000499Z",
     "iopub.status.idle": "2024-10-18T10:20:01.010866Z",
     "shell.execute_reply": "2024-10-18T10:20:01.009952Z",
     "shell.execute_reply.started": "2024-10-18T10:20:01.000773Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def add_quotes(value):\n",
    "    # Check if the value is not already enclosed in double quotes\n",
    "    if not (isinstance(value, str) and value.startswith('\"') and value.endswith('\"')):\n",
    "        return f'\"{value}\"'  # Add double quotes\n",
    "    return value "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-18T10:20:01.012317Z",
     "iopub.status.busy": "2024-10-18T10:20:01.011934Z",
     "iopub.status.idle": "2024-10-18T10:20:01.021010Z",
     "shell.execute_reply": "2024-10-18T10:20:01.020091Z",
     "shell.execute_reply.started": "2024-10-18T10:20:01.012285Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def remove_quotes(value):\n",
    "    # Check if the value is a string\n",
    "    if isinstance(value, str):\n",
    "        # Remove leading and trailing double quotes\n",
    "        return value.strip('\"')\n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-18T10:20:01.022320Z",
     "iopub.status.busy": "2024-10-18T10:20:01.022009Z",
     "iopub.status.idle": "2024-10-18T10:20:01.035674Z",
     "shell.execute_reply": "2024-10-18T10:20:01.034806Z",
     "shell.execute_reply.started": "2024-10-18T10:20:01.022264Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    unique Id                              prediction  \\\n",
      "0     P_1_L_1  crio señor por sus rectifsimos y o cul   \n",
      "1     P_1_L_2       tos juyrios de darme tiempo, para   \n",
      "2     P_1_L_3        que en ruestra educacion lo mani   \n",
      "3     P_1_L_4    tellalle, pues me hallo tantos meses   \n",
      "4     P_1_L_5       ha tendida en esta cama, á vna en   \n",
      "..        ...                                     ...   \n",
      "163  P_7_L_20         Ciceron del Noble, y creed, que   \n",
      "164  P_7_L_21     son talentos de que le aueys de dar   \n",
      "165  P_7_L_22        cuenta muy estrecha, y carga con   \n",
      "166  P_7_L_23       que os ha obligado á ser defenfor   \n",
      "167  P_7_L_24     de su Fū y Religion, accerrimo per-   \n",
      "\n",
      "                postprocessed_transcription  \n",
      "0    crio señor por sus rectifsimos y o cul  \n",
      "1         tos juyrios de darme tiempo, para  \n",
      "2          que en ruestra educacion lo mani  \n",
      "3      tellalle, pues me hallo tantos meses  \n",
      "4         ha tendida en esta cama, a vna en  \n",
      "..                                      ...  \n",
      "163         Ciceron del Noble, y creed, que  \n",
      "164     son talentos de que le aueys de dar  \n",
      "165        cuenta muy estrecha, y carga con  \n",
      "166       que os ha obligado a ser defenfor  \n",
      "167     de su Fū y Religion, accerrimo per-  \n",
      "\n",
      "[168 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "predictions_df['prediction']=predictions_df['prediction'].apply(remove_quotes)\n",
    "print(predictions_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-18T10:20:01.036963Z",
     "iopub.status.busy": "2024-10-18T10:20:01.036644Z",
     "iopub.status.idle": "2024-10-18T10:20:01.044855Z",
     "shell.execute_reply": "2024-10-18T10:20:01.044061Z",
     "shell.execute_reply.started": "2024-10-18T10:20:01.036932Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "predictions_df = predictions_df.drop('prediction', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-18T10:20:01.046125Z",
     "iopub.status.busy": "2024-10-18T10:20:01.045861Z",
     "iopub.status.idle": "2024-10-18T10:20:01.057735Z",
     "shell.execute_reply": "2024-10-18T10:20:01.056962Z",
     "shell.execute_reply.started": "2024-10-18T10:20:01.046096Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "predictions_df = predictions_df.rename(columns={'postprocessed_transcription': 'prediction'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-18T10:20:01.059174Z",
     "iopub.status.busy": "2024-10-18T10:20:01.058860Z",
     "iopub.status.idle": "2024-10-18T10:20:01.071214Z",
     "shell.execute_reply": "2024-10-18T10:20:01.070209Z",
     "shell.execute_reply.started": "2024-10-18T10:20:01.059118Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique Id</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>P_1_L_1</td>\n",
       "      <td>crio señor por sus rectifsimos y o cul</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>P_1_L_2</td>\n",
       "      <td>tos juyrios de darme tiempo, para</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>P_1_L_3</td>\n",
       "      <td>que en ruestra educacion lo mani</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>P_1_L_4</td>\n",
       "      <td>tellalle, pues me hallo tantos meses</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>P_1_L_5</td>\n",
       "      <td>ha tendida en esta cama, a vna en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  unique Id                              prediction\n",
       "0   P_1_L_1  crio señor por sus rectifsimos y o cul\n",
       "1   P_1_L_2       tos juyrios de darme tiempo, para\n",
       "2   P_1_L_3        que en ruestra educacion lo mani\n",
       "3   P_1_L_4    tellalle, pues me hallo tantos meses\n",
       "4   P_1_L_5       ha tendida en esta cama, a vna en"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-18T10:20:01.073612Z",
     "iopub.status.busy": "2024-10-18T10:20:01.072400Z",
     "iopub.status.idle": "2024-10-18T10:20:01.082245Z",
     "shell.execute_reply": "2024-10-18T10:20:01.081363Z",
     "shell.execute_reply.started": "2024-10-18T10:20:01.073551Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "predictions_df.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 9813435,
     "sourceId": 86284,
     "sourceType": "competition"
    },
    {
     "datasetId": 5900333,
     "sourceId": 9658212,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
