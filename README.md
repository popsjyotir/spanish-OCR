# ğŸ“ Spanish-OCR

## ğŸ“Œ Overview
This project focuses on building an OCR (Optical Character Recognition) system capable of transcribing text from **old Spanish manuscripts and scanned documents**. The goal was to develop a model that could handle noisy historical texts, special characters, and poor image quality â€” all common challenges in real-world archival digitization tasks.

---

## ğŸš€ Our Approach

### 1ï¸âƒ£ Baseline: TrOCR-Small
- Started with **TrOCR-small-spanish** on Kaggle (P100 GPU).
- Initial WER (Word Error Rate): **0.78**

---

### 2ï¸âƒ£ Image Binarization
- Applied **binarization** to enhance text visibility by converting grayscale to black-and-white using a pixel threshold.
- Helped reduce WER to **0.6**

---

### 3ï¸âƒ£ Upgrade to TrOCR-Large
- Switched to **TrOCR-Large** for better performance.
- Trained with **batch size = 4** due to GPU limits.
- Results:
  - After 2 epochs: **WER = 0.28**
  - After 14 epochs: **WER = 0.234**

---

### 4ï¸âƒ£ Postprocessing for Common Errors
- Corrected common OCR mistakes (e.g., misrecognized **Ã£**).
- Postprocessing lowered WER further to **0.202**

---

### 5ï¸âƒ£ BART as a Refinement Layer
- Fed TrOCR outputs into a **BART model** trained on ground truth.
- Aimed to fix misrecognized words and improve sentence fluency.
- Due to limited compute, BART didnâ€™t outperform TrOCR-Large.

---

## ğŸ“Š Results

| Model Stage         | WER     |
|---------------------|---------|
| TrOCR-Small         | 0.780   |
| + Binarization      | 0.600   |
| TrOCR-Large         | 0.234   |
| + Postprocessing    | **0.202** âœ… |

**Final Model WER:** 0.202  

---

## ğŸ’¡ Challenges & Learnings

- ğŸ§  **Limited GPU** = smaller batch sizes & slower training  
- ğŸ”£ **Rare characters** were misrecognized due to limited training data  
- ğŸ› ï¸ **Postprocessing** made a meaningful difference

---

## âœ… Conclusion & Future Work

Despite hardware constraints, we achieved strong results through:
- Careful preprocessing
- Model scaling
- Smart postprocessing

ğŸ”® Next steps:
- Explore **multi-stage refinement**
- Enhance **training data diversity**
- Utilize **stronger GPUs** for larger models

---
