{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":86284,"databundleVersionId":9813435,"sourceType":"competition"}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install jiwer","metadata":{"execution":{"iopub.status.busy":"2024-10-17T08:15:31.409838Z","iopub.execute_input":"2024-10-17T08:15:31.410245Z","iopub.status.idle":"2024-10-17T08:15:46.316172Z","shell.execute_reply.started":"2024-10-17T08:15:31.410176Z","shell.execute_reply":"2024-10-17T08:15:46.314703Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Collecting jiwer\n  Downloading jiwer-3.0.4-py3-none-any.whl.metadata (2.6 kB)\nRequirement already satisfied: click<9.0.0,>=8.1.3 in /opt/conda/lib/python3.10/site-packages (from jiwer) (8.1.7)\nCollecting rapidfuzz<4,>=3 (from jiwer)\n  Downloading rapidfuzz-3.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\nDownloading jiwer-3.0.4-py3-none-any.whl (21 kB)\nDownloading rapidfuzz-3.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: rapidfuzz, jiwer\nSuccessfully installed jiwer-3.0.4 rapidfuzz-3.10.0\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install --upgrade transformers","metadata":{"execution":{"iopub.status.busy":"2024-10-17T08:15:46.318131Z","iopub.execute_input":"2024-10-17T08:15:46.318517Z","iopub.status.idle":"2024-10-17T08:16:11.858842Z","shell.execute_reply.started":"2024-10-17T08:15:46.318479Z","shell.execute_reply":"2024-10-17T08:16:11.857471Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.45.1)\nCollecting transformers\n  Downloading transformers-4.45.2-py3-none-any.whl.metadata (44 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.15.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.25.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.5)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.20.0)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.8.30)\nDownloading transformers-4.45.2-py3-none-any.whl (9.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m54.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: transformers\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.45.1\n    Uninstalling transformers-4.45.1:\n      Successfully uninstalled transformers-4.45.1\nSuccessfully installed transformers-4.45.2\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import torch\nfrom transformers import TrOCRProcessor, VisionEncoderDecoderModel\nfrom transformers import Trainer, TrainingArguments\nfrom transformers import VisionEncoderDecoderModel, AutoProcessor\nfrom datasets import load_dataset\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport os\nimport numpy as np\nimport pandas as pd\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision import transforms\nfrom sklearn.model_selection import train_test_split\nfrom tqdm.notebook import tqdm_notebook as tqdm\nfrom transformers import AdamW\nimport torch.nn.functional as F\ndevice='cuda' if torch.cuda.is_available() else 'cpu'\nfrom jiwer import wer","metadata":{"execution":{"iopub.status.busy":"2024-10-17T08:16:11.860474Z","iopub.execute_input":"2024-10-17T08:16:11.860836Z","iopub.status.idle":"2024-10-17T08:16:35.643386Z","shell.execute_reply.started":"2024-10-17T08:16:11.860798Z","shell.execute_reply":"2024-10-17T08:16:35.642056Z"},"trusted":true},"outputs":[],"execution_count":3},{"cell_type":"code","source":"class Customdataset(Dataset):\n    def __init__(self, li):\n        super().__init__()\n        self.li=li\n    def __len__(self):\n        return len(self.li)\n    def __getitem__(self,idx):\n        path=self.li[idx][0]\n        y=self.li[idx][1]\n        return {\n            'image':path,\n            'text':y\n        }","metadata":{"execution":{"iopub.status.busy":"2024-10-17T08:16:35.646390Z","iopub.execute_input":"2024-10-17T08:16:35.647286Z","iopub.status.idle":"2024-10-17T08:16:35.653907Z","shell.execute_reply.started":"2024-10-17T08:16:35.647200Z","shell.execute_reply":"2024-10-17T08:16:35.652695Z"},"trusted":true},"outputs":[],"execution_count":4},{"cell_type":"code","source":"li = []\nimage_path = '/kaggle/input/ai-of-god-3/Public_data/train_images'\ncsv_path = '/kaggle/input/ai-of-god-3/Public_data/train.csv'\n\ndf = pd.read_csv(csv_path)\n\nfor index, row in tqdm(df.iterrows()):\n    full_path = os.path.join(image_path, row['unique Id'] + '.png')\n    if os.path.isfile(full_path):\n        li.append([full_path, row['transcription']])","metadata":{"execution":{"iopub.status.busy":"2024-10-17T08:16:35.655357Z","iopub.execute_input":"2024-10-17T08:16:35.656089Z","iopub.status.idle":"2024-10-17T08:17:17.060696Z","shell.execute_reply.started":"2024-10-17T08:16:35.656038Z","shell.execute_reply":"2024-10-17T08:17:17.059308Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4adb43982f10402989e4e31049c64992"}},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"data_train,data_val=train_test_split(li,train_size=0.8, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2024-10-17T08:17:17.062378Z","iopub.execute_input":"2024-10-17T08:17:17.062785Z","iopub.status.idle":"2024-10-17T08:17:17.075768Z","shell.execute_reply.started":"2024-10-17T08:17:17.062745Z","shell.execute_reply":"2024-10-17T08:17:17.074664Z"},"trusted":true},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# Load the model and processor\nmodel = VisionEncoderDecoderModel.from_pretrained(\"qantev/trocr-small-spanish\")\nprocessor = AutoProcessor.from_pretrained(\"qantev/trocr-small-spanish\")\ntokenizer = TrOCRProcessor.from_pretrained('qantev/trocr-small-spanish')","metadata":{"execution":{"iopub.status.busy":"2024-10-17T08:17:17.077155Z","iopub.execute_input":"2024-10-17T08:17:17.077535Z","iopub.status.idle":"2024-10-17T08:17:28.232490Z","shell.execute_reply.started":"2024-10-17T08:17:17.077499Z","shell.execute_reply":"2024-10-17T08:17:28.231267Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/4.82k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1aa4f60f81cb47ad96d6b2013a9865e9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/247M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f10d14dad5fe4388b456fd255c6b8ef3"}},"metadata":{}},{"name":"stderr","text":"VisionEncoderDecoderModel has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/384 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8276e6001d6140f5834ef200c5bca78a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/450 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"49fc5dcdece74720a819d4d9ea01f6f6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/510 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3d0d8f51f586462689fe7d2e914d54af"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentencepiece.bpe.model:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d00bde300e864ff3a40c91ebe1ced8a8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/4.49M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"780a4ea5c99e4d38aa606e8fdfc817c8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/279 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f925ff1d9772418c9e5c463977cf739e"}},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"train_data=Customdataset(data_train)\nval_data=Customdataset(data_val)","metadata":{"execution":{"iopub.status.busy":"2024-10-17T08:17:28.234126Z","iopub.execute_input":"2024-10-17T08:17:28.234524Z","iopub.status.idle":"2024-10-17T08:17:28.239623Z","shell.execute_reply.started":"2024-10-17T08:17:28.234487Z","shell.execute_reply":"2024-10-17T08:17:28.238430Z"},"trusted":true},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# def preprocess_batch(batch):\n#     preprocess_pipeline = transforms.Compose([\n#         transforms.Grayscale(num_output_channels=3),  # Convert to grayscale with 3 channels\n#         transforms.ToTensor()\n#     ])\n\n#     processed_images = []\n#     threshold = 0.6875  # Set your threshold\n\n#     # Process each image in the batch\n#     for image_path in batch['image']:\n#         # Open the image\n#         image = Image.open(image_path).convert(\"RGB\")\n        \n#         # Apply the grayscale transformation\n#         grayscale_image = preprocess_pipeline(image)\n\n#         # Convert to numpy for pixel manipulation\n#         grayscale_np = grayscale_image.numpy()\n\n#         # Create a mask where pixel values are less than the threshold\n#         mask = grayscale_np < threshold\n\n#         # Turn those pixels black (0.0) and the others white (1.0)\n#         grayscale_np[mask] = 0.0      # Text will be black\n#         grayscale_np[~mask] = 1.0     # Background will be white\n\n#         # Convert back to tensor and add to the list\n#         processed_image = torch.tensor(grayscale_np)\n#         processed_images.append(processed_image)\n\n#     # Stack processed images into a single tensor (N, C, H, W)\n#     pixel_values = torch.cat([processor(image, return_tensors=\"pt\").pixel_values for image in batch['image']], dim=0)\n\n#     # Process text and tokenize labels\n#     labels = torch.cat([\n#         processor.tokenizer(text, padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"pt\").input_ids\n#         for text in batch['text']\n#     ], dim=0)\n\n#     return {\"pixel_values\": pixel_values, \"labels\": labels}","metadata":{"execution":{"iopub.status.busy":"2024-10-17T08:23:54.165245Z","iopub.execute_input":"2024-10-17T08:23:54.165734Z","iopub.status.idle":"2024-10-17T08:23:54.175374Z","shell.execute_reply.started":"2024-10-17T08:23:54.165694Z","shell.execute_reply":"2024-10-17T08:23:54.174243Z"},"trusted":true},"outputs":[],"execution_count":20},{"cell_type":"code","source":"def preprocess_batch(batch, threshold=0.6875):\n    # Define the preprocessing pipeline\n    preprocess_pipeline = transforms.Compose([\n        transforms.Grayscale(num_output_channels=3),  # Convert image to grayscale with 3 channels\n        transforms.ToTensor(),  # Convert image to tensor\n    ])\n\n    # Process images in batch\n    images = [Image.open(image_path).convert(\"RGB\") for image_path in batch['image']]\n\n    # Parallelize transformation using torch's stack\n    grayscale_images = torch.cat([preprocess_pipeline(image).unsqueeze(0) for image in images], dim=0)\n    \n    grayscale_images = grayscale_images.to(device)\n\n    # Apply thresholding\n    mask = grayscale_images < threshold\n    grayscale_images[mask] = 0.0  # Black for values below threshold\n    grayscale_images[~mask] = 1.0  # White for values above threshold\n\n    # Use the processor to process the images\n    pixel_values = processor(grayscale_images, return_tensors=\"pt\", do_rescale=False).pixel_values\n\n    # Tokenize text labels in the batch\n    labels = processor.tokenizer(\n        batch['text'], padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"pt\"\n    ).input_ids\n\n    return {\"pixel_values\": pixel_values, \"labels\": labels}","metadata":{"execution":{"iopub.status.busy":"2024-10-17T08:29:47.423733Z","iopub.execute_input":"2024-10-17T08:29:47.424625Z","iopub.status.idle":"2024-10-17T08:29:47.433507Z","shell.execute_reply.started":"2024-10-17T08:29:47.424578Z","shell.execute_reply":"2024-10-17T08:29:47.432129Z"},"trusted":true},"outputs":[],"execution_count":22},{"cell_type":"code","source":"# Define the preprocessing function\ndef preprocess_image(image_path, threshold=0.6875):\n    # Open the image\n    image = Image.open(image_path).convert(\"RGB\")\n\n    # Apply the grayscale transformation\n    transform = transforms.Compose([\n        transforms.Grayscale(num_output_channels=3),  # Convert to grayscale with 3 channels\n        transforms.ToTensor()\n    ])\n\n    # Apply the transformations\n    grayscale_image = transform(image)\n\n    # Convert to numpy for pixel manipulation\n    grayscale_np = grayscale_image.numpy()\n\n    # Create a mask where pixel values are less than the threshold\n    mask = grayscale_np < threshold\n\n    # Turn those pixels black (0.0) and the others white (1.0)\n    grayscale_np[mask] = 0.0      # Text will be black\n    grayscale_np[~mask] = 1.0     # Background will be white\n\n    # Convert back to tensor\n    processed_image = torch.tensor(grayscale_np)\n\n    return processed_image","metadata":{"execution":{"iopub.status.busy":"2024-10-17T08:17:28.304247Z","iopub.execute_input":"2024-10-17T08:17:28.304652Z","iopub.status.idle":"2024-10-17T08:17:28.320260Z","shell.execute_reply.started":"2024-10-17T08:17:28.304615Z","shell.execute_reply":"2024-10-17T08:17:28.319081Z"},"trusted":true},"outputs":[],"execution_count":11},{"cell_type":"code","source":"train_loader = DataLoader(train_data, batch_size=8, shuffle=True)\nval_loader = DataLoader(val_data, batch_size=8, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2024-10-17T08:17:28.321706Z","iopub.execute_input":"2024-10-17T08:17:28.322114Z","iopub.status.idle":"2024-10-17T08:17:28.337311Z","shell.execute_reply.started":"2024-10-17T08:17:28.322074Z","shell.execute_reply":"2024-10-17T08:17:28.336157Z"},"trusted":true},"outputs":[],"execution_count":12},{"cell_type":"code","source":"import numpy as np\n\ndef wer(reference, hypothesis):\n    \"\"\"\n    Calculate the Word Error Rate (WER) between a reference and hypothesis sentence.\n    WER is defined as the minimum number of word-level edits (insertions, deletions, substitutions)\n    required to convert the hypothesis sentence into the reference sentence, divided by the total number of words in the reference.\n\n    Parameters:\n    reference (str): The ground truth sentence.\n    hypothesis (str): The predicted sentence.\n\n    Returns:\n    float: The word error rate between the reference and hypothesis.\n    \"\"\"\n\n    # Split the reference and hypothesis sentences into words\n    reference_words = reference.split()\n    hypothesis_words = hypothesis.split()\n\n    # Create a matrix to store the distances\n    d = np.zeros((len(reference_words) + 1, len(hypothesis_words) + 1), dtype=np.uint8)\n\n    # Initialize the matrix\n    for i in range(1, len(reference_words) + 1):\n        d[i][0] = i\n    for j in range(1, len(hypothesis_words) + 1):\n        d[0][j] = j\n\n    # Fill the matrix\n    for i in range(1, len(reference_words) + 1):\n        for j in range(1, len(hypothesis_words) + 1):\n            if reference_words[i - 1] == hypothesis_words[j - 1]:\n                d[i][j] = d[i - 1][j - 1]\n            else:\n                d[i][j] = min(d[i - 1][j], d[i][j - 1], d[i - 1][j - 1]) + 1\n\n    # The WER is the number of errors divided by the total number of words in the reference sentence\n    wer_value = d[len(reference_words)][len(hypothesis_words)] / len(reference_words)\n\n    return wer_value","metadata":{"execution":{"iopub.status.busy":"2024-10-17T08:17:28.338950Z","iopub.execute_input":"2024-10-17T08:17:28.339791Z","iopub.status.idle":"2024-10-17T08:17:28.350298Z","shell.execute_reply.started":"2024-10-17T08:17:28.339736Z","shell.execute_reply":"2024-10-17T08:17:28.349230Z"},"trusted":true},"outputs":[],"execution_count":13},{"cell_type":"code","source":"def avg_wer(ground_truths, predictions):\n    wer_loss=0\n    for reference, hypothesis in zip(ground_truths, predictions):\n        wer_loss += wer(reference, hypothesis)\n    return wer_loss/len(ground_truths)","metadata":{"execution":{"iopub.status.busy":"2024-10-17T08:17:28.351704Z","iopub.execute_input":"2024-10-17T08:17:28.352113Z","iopub.status.idle":"2024-10-17T08:17:28.366894Z","shell.execute_reply.started":"2024-10-17T08:17:28.352078Z","shell.execute_reply":"2024-10-17T08:17:28.365782Z"},"trusted":true},"outputs":[],"execution_count":14},{"cell_type":"code","source":"model = model.to(device)\n\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n\nnum_epochs = 14","metadata":{"execution":{"iopub.status.busy":"2024-10-17T08:17:28.368363Z","iopub.execute_input":"2024-10-17T08:17:28.368792Z","iopub.status.idle":"2024-10-17T08:17:28.389468Z","shell.execute_reply.started":"2024-10-17T08:17:28.368754Z","shell.execute_reply":"2024-10-17T08:17:28.388297Z"},"trusted":true},"outputs":[],"execution_count":15},{"cell_type":"code","source":"LR_SCHEDULER = torch.optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.1)","metadata":{"execution":{"iopub.status.busy":"2024-10-17T08:17:28.391088Z","iopub.execute_input":"2024-10-17T08:17:28.391469Z","iopub.status.idle":"2024-10-17T08:17:28.396185Z","shell.execute_reply.started":"2024-10-17T08:17:28.391432Z","shell.execute_reply":"2024-10-17T08:17:28.395140Z"},"trusted":true},"outputs":[],"execution_count":16},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2024-10-17T08:17:28.397315Z","iopub.execute_input":"2024-10-17T08:17:28.397654Z","iopub.status.idle":"2024-10-17T08:17:28.408565Z","shell.execute_reply.started":"2024-10-17T08:17:28.397612Z","shell.execute_reply":"2024-10-17T08:17:28.407295Z"},"trusted":true},"outputs":[],"execution_count":17},{"cell_type":"code","source":"# for epoch in range(num_epochs):\n#     print(f\"Epoch {epoch + 1}/{num_epochs}\")\n\n#     model.train()\n    \n#     predictions = []\n#     ground_truths = []\n\n#     for batch in tqdm(train_loader, desc=\"Training Batches\"):\n#         processed_batch = preprocess_batch(batch)\n        \n#         pixel_values = processed_batch['pixel_values'].to(device)\n#         labels = processed_batch['labels'].to(device)\n\n#         outputs = model(pixel_values=pixel_values, labels=labels)\n        \n#         predicted_ids = outputs.logits.argmax(dim=-1)\n#         predicted_strings = tokenizer.batch_decode(predicted_ids, skip_special_tokens=True)\n\n#         true_ids = labels.detach().cpu().numpy()\n#         true_strings = tokenizer.batch_decode(true_ids, skip_special_tokens=True)\n\n#         predictions.extend(predicted_strings)\n#         ground_truths.extend(true_strings)\n\n#         loss = outputs.loss\n\n#         loss.backward()\n#         optimizer.step()\n#         optimizer.zero_grad()\n\n#     avg_train_wer_loss = avg_wer(ground_truths, predictions)\n#     print(f\"Average Training WER Loss: {avg_train_wer_loss:.4f}\")\n#     LR_SCHEDULER.step()\n    \n#     predictions = []\n#     ground_truths = []\n\n#     model.eval()\n\n#     with torch.no_grad():\n#         for batch in tqdm(val_loader, desc=\"Validation Batches\"):\n#             processed_batch = preprocess_batch(batch)\n#             pixel_values = processed_batch['pixel_values'].to(device)\n#             labels = processed_batch['labels'].to(device)\n\n#             outputs = model(pixel_values=pixel_values, labels=labels)\n            \n#             predicted_ids = outputs.logits.argmax(dim=-1)\n#             predicted_strings = tokenizer.batch_decode(predicted_ids, skip_special_tokens=True)\n\n#             true_ids = labels.detach().cpu().numpy()\n#             true_strings = tokenizer.batch_decode(true_ids, skip_special_tokens=True)\n\n#             predictions.extend(predicted_strings)\n#             ground_truths.extend(true_strings)\n\n#     avg_val_wer_loss = avg_wer(ground_truths, predictions)\n#     print(f\"Average Validation WER Loss: {avg_val_wer_loss:.4f}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-17T08:17:28.410137Z","iopub.execute_input":"2024-10-17T08:17:28.410617Z","iopub.status.idle":"2024-10-17T08:17:28.421381Z","shell.execute_reply.started":"2024-10-17T08:17:28.410565Z","shell.execute_reply":"2024-10-17T08:17:28.420292Z"},"trusted":true},"outputs":[],"execution_count":18},{"cell_type":"code","source":"for epoch in range(num_epochs):\n    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n\n    # Set the model to training mode\n    model.train()\n\n    predictions = []\n    ground_truths = []\n\n    for batch in tqdm(train_loader, desc=\"Training Batches\"):\n        # Preprocess the batch (apply transformations and tokenization)\n        processed_batch = preprocess_batch(batch)\n\n        # Move pixel values and labels to the GPU\n        pixel_values = processed_batch['pixel_values'].to(device)\n        labels = processed_batch['labels'].to(device)\n\n        # Perform forward pass (compute logits and loss)\n        outputs = model(pixel_values=pixel_values, labels=labels)\n\n        # Get the predicted token IDs\n        predicted_ids = outputs.logits.argmax(dim=-1)\n\n        # Decode the predicted token IDs into strings\n        predicted_strings = tokenizer.batch_decode(predicted_ids, skip_special_tokens=True)\n\n        # Decode the ground truth labels into strings\n        true_ids = labels.detach().cpu().numpy()\n        true_strings = tokenizer.batch_decode(true_ids, skip_special_tokens=True)\n\n        # Collect the predictions and ground truths\n        predictions.extend(predicted_strings)\n        ground_truths.extend(true_strings)\n\n        # Compute loss and backpropagate\n        loss = outputs.loss\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n    # Calculate and print the Average Word Error Rate (WER) loss for training\n    avg_train_wer_loss = avg_wer(ground_truths, predictions)\n    print(f\"Average Training WER Loss: {avg_train_wer_loss:.4f}\")\n\n    # Update the learning rate scheduler\n    LR_SCHEDULER.step()\n\n    # Validation Phase\n    predictions = []\n    ground_truths = []\n\n    # Set the model to evaluation mode\n    model.eval()\n\n    with torch.no_grad():\n        for batch in tqdm(val_loader, desc=\"Validation Batches\"):\n            # Preprocess the batch for validation\n            processed_batch = preprocess_batch(batch)\n\n            # Move pixel values and labels to the GPU\n            pixel_values = processed_batch['pixel_values'].to(device)\n            labels = processed_batch['labels'].to(device)\n\n            # Perform forward pass (compute logits, no backpropagation)\n            outputs = model(pixel_values=pixel_values, labels=labels)\n\n            # Get the predicted token IDs\n            predicted_ids = outputs.logits.argmax(dim=-1)\n\n            # Decode the predicted token IDs into strings\n            predicted_strings = tokenizer.batch_decode(predicted_ids, skip_special_tokens=True)\n\n            # Decode the ground truth labels into strings\n            true_ids = labels.detach().cpu().numpy()\n            true_strings = tokenizer.batch_decode(true_ids, skip_special_tokens=True)\n\n            # Collect the predictions and ground truths\n            predictions.extend(predicted_strings)\n            ground_truths.extend(true_strings)\n\n    # Calculate and print the Average WER loss for validation\n    avg_val_wer_loss = avg_wer(ground_truths, predictions)\n    print(f\"Average Validation WER Loss: {avg_val_wer_loss:.4f}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-17T08:29:55.653478Z","iopub.execute_input":"2024-10-17T08:29:55.653911Z","iopub.status.idle":"2024-10-17T08:29:55.850588Z","shell.execute_reply.started":"2024-10-17T08:29:55.653869Z","shell.execute_reply":"2024-10-17T08:29:55.848937Z"},"trusted":true,"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Epoch 1/14\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training Batches:   0%|          | 0/1501 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2f7c98899ae341149ffcd1b5e1e24317"}},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[23], line 12\u001b[0m\n\u001b[1;32m      8\u001b[0m ground_truths \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m tqdm(train_loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining Batches\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# Preprocess the batch (apply transformations and tokenization)\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m     processed_batch \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# Move pixel values and labels to the GPU\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     pixel_values \u001b[38;5;241m=\u001b[39m processed_batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpixel_values\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n","Cell \u001b[0;32mIn[22], line 12\u001b[0m, in \u001b[0;36mpreprocess_batch\u001b[0;34m(batch, threshold)\u001b[0m\n\u001b[1;32m      9\u001b[0m images \u001b[38;5;241m=\u001b[39m [Image\u001b[38;5;241m.\u001b[39mopen(image_path)\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m image_path \u001b[38;5;129;01min\u001b[39;00m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Parallelize transformation using torch's stack\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m grayscale_images \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpreprocess_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m grayscale_images \u001b[38;5;241m=\u001b[39m grayscale_images\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Apply thresholding\u001b[39;00m\n","\u001b[0;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 0. Expected size 712 but got size 706 for tensor number 1 in the list."],"ename":"RuntimeError","evalue":"Sizes of tensors must match except in dimension 0. Expected size 712 but got size 706 for tensor number 1 in the list.","output_type":"error"}],"execution_count":23},{"cell_type":"code","source":"test_df = pd.read_csv('/kaggle/input/ai-of-god-3/Public_data/test.csv')","metadata":{"execution":{"iopub.status.busy":"2024-10-17T08:17:29.283206Z","iopub.status.idle":"2024-10-17T08:17:29.283619Z","shell.execute_reply.started":"2024-10-17T08:17:29.283429Z","shell.execute_reply":"2024-10-17T08:17:29.283450Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_path = '/kaggle/input/ai-of-god-3/Public_data/test_images'\n\n# Function to convert unique ID into correct image path\ndef construct_image_path(unique_id):\n    parts = unique_id.split('_')\n    page_number = parts[1]\n    l_number = parts[3] \n    \n    image_path = os.path.join(test_path, f'Page_{page_number}', f'L_{l_number}.png')\n    return image_path\n\ntest_df['image_path'] = test_df['unique Id'].apply(construct_image_path)\n\n# Check the resulting dataframe\nprint(test_df[[ 'image_path']].head())","metadata":{"execution":{"iopub.status.busy":"2024-10-17T08:17:29.284906Z","iopub.status.idle":"2024-10-17T08:17:29.285429Z","shell.execute_reply.started":"2024-10-17T08:17:29.285208Z","shell.execute_reply":"2024-10-17T08:17:29.285244Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"predictions = []\nmodel.eval()\n\nwith torch.no_grad():\n    for _, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"Inference\"):\n        image_path = row['image_path']\n        \n        # Preprocess the image using the defined function\n        pixel_values = preprocess_image(image_path).to(device)  # Add batch dimension and move to device\n        \n        # Generate output\n        outputs = model.generate(pixel_values)\n\n        # Decode the predicted string\n        predicted_string = tokenizer.decode(outputs[0], skip_special_tokens=True)\n        \n        # Append the results\n        predictions.append({\n            'unique Id': row['unique Id'],  # Ensure 'unique Id' is correctly named in the dataframe\n            'prediction': predicted_string\n        })\n\n# Create DataFrame for predictions\npredictions_df = pd.DataFrame(predictions)","metadata":{"execution":{"iopub.status.busy":"2024-10-17T08:17:29.286436Z","iopub.status.idle":"2024-10-17T08:17:29.286860Z","shell.execute_reply.started":"2024-10-17T08:17:29.286650Z","shell.execute_reply":"2024-10-17T08:17:29.286677Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# predictions = []\n\n# model.eval()\n\n# with torch.no_grad():\n#     for _, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"Inference\"):\n#         image_path = row['image_path']\n        \n#         # Open and preprocess the image (convert to grayscale, normalize, etc.)\n#         image = Image.open(image_path).convert(\"RGB\")\n        \n#         # Apply the transformations (grayscale, normalize, and ToTensor)\n#         transform = transforms.Compose([\n#             transforms.Grayscale(num_output_channels=3),\n#             transforms.ToTensor()\n#         ])\n        \n#         image = transform(image)\n        \n#         # Process the image using the processor\n#         pixel_values = processor(image, return_tensors=\"pt\").pixel_values.to(device)\n        \n#         # Generate output\n#         outputs = model.generate(pixel_values)\n\n#         # Decode the predicted string\n#         predicted_string = tokenizer.decode(outputs[0], skip_special_tokens=True)\n        \n#         # Append the results\n#         predictions.append({\n#             'unique Id': row['unique Id'],  # Ensure 'unique Id' is correctly named in the dataframe\n#             'prediction': predicted_string\n#         })\n\n# # Create DataFrame for predictions\n# predictions_df = pd.DataFrame(predictions)","metadata":{"execution":{"iopub.status.busy":"2024-10-17T08:17:29.288393Z","iopub.status.idle":"2024-10-17T08:17:29.288937Z","shell.execute_reply.started":"2024-10-17T08:17:29.288659Z","shell.execute_reply":"2024-10-17T08:17:29.288687Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"predictions_df.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-10-17T08:17:29.290671Z","iopub.status.idle":"2024-10-17T08:17:29.291239Z","shell.execute_reply.started":"2024-10-17T08:17:29.290926Z","shell.execute_reply":"2024-10-17T08:17:29.290956Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"predictions_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-10-17T08:17:29.292434Z","iopub.status.idle":"2024-10-17T08:17:29.292973Z","shell.execute_reply.started":"2024-10-17T08:17:29.292689Z","shell.execute_reply":"2024-10-17T08:17:29.292719Z"},"trusted":true},"outputs":[],"execution_count":null}]}