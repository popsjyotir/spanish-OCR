{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":86284,"databundleVersionId":9813435,"sourceType":"competition"},{"sourceId":9647790,"sourceType":"datasetVersion","datasetId":5892094}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install jiwer","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-10-19T09:34:04.890091Z","iopub.execute_input":"2024-10-19T09:34:04.890628Z","iopub.status.idle":"2024-10-19T09:34:22.727714Z","shell.execute_reply.started":"2024-10-19T09:34:04.890569Z","shell.execute_reply":"2024-10-19T09:34:22.726020Z"}},"outputs":[{"name":"stdout","text":"Collecting jiwer\n  Downloading jiwer-3.0.4-py3-none-any.whl.metadata (2.6 kB)\nRequirement already satisfied: click<9.0.0,>=8.1.3 in /opt/conda/lib/python3.10/site-packages (from jiwer) (8.1.7)\nCollecting rapidfuzz<4,>=3 (from jiwer)\n  Downloading rapidfuzz-3.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\nDownloading jiwer-3.0.4-py3-none-any.whl (21 kB)\nDownloading rapidfuzz-3.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m36.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: rapidfuzz, jiwer\nSuccessfully installed jiwer-3.0.4 rapidfuzz-3.10.0\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!pip install --upgrade tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-19T09:34:22.730343Z","iopub.execute_input":"2024-10-19T09:34:22.730737Z","iopub.status.idle":"2024-10-19T09:34:37.318670Z","shell.execute_reply.started":"2024-10-19T09:34:22.730694Z","shell.execute_reply":"2024-10-19T09:34:37.317309Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (4.66.4)\nCollecting tqdm\n  Downloading tqdm-4.66.5-py3-none-any.whl.metadata (57 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tqdm-4.66.5-py3-none-any.whl (78 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.4/78.4 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: tqdm\n  Attempting uninstall: tqdm\n    Found existing installation: tqdm 4.66.4\n    Uninstalling tqdm-4.66.4:\n      Successfully uninstalled tqdm-4.66.4\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbeatrix-jupyterlab 2024.66.154055 requires jupyterlab~=3.6.0, but you have jupyterlab 4.2.5 which is incompatible.\nconda 24.9.0 requires packaging>=23.0, but you have packaging 21.3 which is incompatible.\nydata-profiling 4.10.0 requires scipy<1.14,>=1.4.1, but you have scipy 1.14.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed tqdm-4.66.5\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"!pip install --upgrade transformers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-19T09:34:37.320855Z","iopub.execute_input":"2024-10-19T09:34:37.321302Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.45.1)\nCollecting transformers\n  Downloading transformers-4.45.2-py3-none-any.whl.metadata (44 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom transformers import TrOCRProcessor, VisionEncoderDecoderModel\nfrom transformers import Trainer, TrainingArguments\nfrom transformers import VisionEncoderDecoderModel, AutoProcessor\nfrom datasets import load_dataset\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport os\nimport numpy as np\nimport pandas as pd\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision import transforms\nfrom sklearn.model_selection import train_test_split\nfrom tqdm.notebook import tqdm_notebook as tqdm\nfrom transformers import AdamW\nimport torch.nn.functional as F\ndevice='cuda' if torch.cuda.is_available() else 'cpu'\nfrom jiwer import wer","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class CustomDataset(Dataset):\n    def __init__(self, li):\n        super().__init__()\n        self.li = li\n    def __len__(self):\n        return len(self.li)\n    def __getitem__(self, idx):\n        path = self.li[idx][0]\n        label = self.li[idx][1]\n        return {\n            'image':path,\n            'text':label\n        }","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"li = []\nimage_path = '/kaggle/input/ai-of-god-3/Public_data/train_images'\ncsv_path = '/kaggle/input/ai-of-god-3/Public_data/train.csv'\n\ndf = pd.read_csv(csv_path)\n\nfor index, row in tqdm(df.iterrows()):\n    full_path = os.path.join(image_path, row['unique Id'] + '.png')\n    if os.path.isfile(full_path):\n        li.append([full_path, row['transcription']])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"extra_image = \"/kaggle/input/extra-spanish-text-1k/train_images/train_images\"\nextra_csv = \"/kaggle/input/extra-spanish-text-1k/train_texts.csv\"\n\ndf1 = pd.read_csv(extra_csv)\nli1= []","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df1.columns.values[0] = 'unique Id'  # Change the first column heading to \"unique Id\"\ndf1 = df1.rename(columns={'transcription': 'nans'})\ndf1 = df1.rename(columns={'unique Id': 'transcription'})","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df1[\"unique Id\"] = range(len(df1))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for index, row in tqdm(df1.iterrows()):\n    # Construct the full path using the 'unique Id' from the current row\n    full_path1 = os.path.join(extra_image, 'transcription_' + str(row['unique Id']) + '.png')\n     # Print the constructed path (for debugging or verification)\n\n    # Check if the file exists, and if so, append the desired data to the list\n    if os.path.isfile(full_path1):\n        li1.append([full_path1, row['transcription']])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n\n# Path to your image\nimage_path = '/path/to/your/image.jpg'\n\n# Load and display the image\nimg = mpimg.imread(image_path)\nplt.imshow(img)\nplt.axis('off')  # To hide the axes\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"lifinal = li + li1","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data_train,data_val=train_test_split(lifinal,train_size=0.8, random_state=42)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_data=CustomDataset(data_train)\nval_data=CustomDataset(data_val)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-17T20:36:10.064399Z","iopub.execute_input":"2024-10-17T20:36:10.064837Z","iopub.status.idle":"2024-10-17T20:36:10.069328Z","shell.execute_reply.started":"2024-10-17T20:36:10.064798Z","shell.execute_reply":"2024-10-17T20:36:10.068300Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# Load the model and processor\nmodel = VisionEncoderDecoderModel.from_pretrained('qantev/trocr-large-spanish')\nprocessor = AutoProcessor.from_pretrained('qantev/trocr-large-spanish')\ntokenizer = TrOCRProcessor.from_pretrained('qantev/trocr-large-spanish')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-17T20:36:11.293360Z","iopub.execute_input":"2024-10-17T20:36:11.294246Z","iopub.status.idle":"2024-10-17T20:36:40.091292Z","shell.execute_reply.started":"2024-10-17T20:36:11.294202Z","shell.execute_reply":"2024-10-17T20:36:40.090346Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/4.97k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"664116f2843142089210cc9296eba871"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/2.44G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"942dbfdf8f0040e8b63d93150ed0f862"}},"metadata":{}},{"name":"stderr","text":"VisionEncoderDecoderModel has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/420 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"19cd2c8279ec4f808f050871fe1c41ec"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/364 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7106bc74619941a6b84805be115485e1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.38k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d4bc0958a4724bbd83e2111e07316579"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fd1a78f9f9124bb4947bd9891bb3d11a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8d74e1bab9c24bf7b79ed7bc03fe1231"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"38344a44d71a49b383f96e9cd3cffa21"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/957 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"585973b614da49ecb6bf2fe396fc095d"}},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"def preprocess_batch(batch, threshold=0.6875):\n    # Define the preprocessing pipeline\n    preprocess_pipeline = transforms.Compose([\n        transforms.Resize(size =(50,700)),\n        transforms.Grayscale(num_output_channels=3),  # Convert image to grayscale with 3 channels\n        transforms.ToTensor(),  # Convert image to tensor\n    ])\n\n    # Process images in batch\n    images = [Image.open(image_path).convert(\"RGB\") for image_path in batch['image']]\n\n    # Parallelize transformation using torch's stack\n    grayscale_images = torch.cat([preprocess_pipeline(image).unsqueeze(0) for image in images], dim=0)\n    \n    grayscale_images = grayscale_images.to(device)\n\n    # Apply thresholding\n    mask = grayscale_images < threshold\n    grayscale_images[mask] = 0.0  # Black for values below threshold\n    grayscale_images[~mask] = 1.0  # White for values above threshold\n\n    # Use the processor to process the images\n    pixel_values = processor(grayscale_images, return_tensors=\"pt\", do_rescale=False).pixel_values\n\n    # Tokenize text labels in the batch\n    labels = processor.tokenizer(\n        batch['text'], padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"pt\"\n    ).input_ids\n\n    return {\"pixel_values\": pixel_values, \"labels\": labels}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-17T20:36:57.271137Z","iopub.execute_input":"2024-10-17T20:36:57.271564Z","iopub.status.idle":"2024-10-17T20:36:57.280446Z","shell.execute_reply.started":"2024-10-17T20:36:57.271522Z","shell.execute_reply":"2024-10-17T20:36:57.279187Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"train_loader = DataLoader(train_data, batch_size=4, shuffle=True)\nval_loader = DataLoader(val_data, batch_size=4, shuffle=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-17T20:36:58.502813Z","iopub.execute_input":"2024-10-17T20:36:58.503246Z","iopub.status.idle":"2024-10-17T20:36:58.508529Z","shell.execute_reply.started":"2024-10-17T20:36:58.503204Z","shell.execute_reply":"2024-10-17T20:36:58.507490Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"batch = next(iter(train_loader))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-19T09:33:58.930220Z","iopub.execute_input":"2024-10-19T09:33:58.931147Z","iopub.status.idle":"2024-10-19T09:33:59.340987Z","shell.execute_reply.started":"2024-10-19T09:33:58.931085Z","shell.execute_reply":"2024-10-19T09:33:59.339209Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(\u001b[43mtrain_loader\u001b[49m))\n","\u001b[0;31mNameError\u001b[0m: name 'train_loader' is not defined"],"ename":"NameError","evalue":"name 'train_loader' is not defined","output_type":"error"}],"execution_count":1},{"cell_type":"code","source":"import numpy as np\n\ndef wer(reference, hypothesis):\n    \"\"\"\n    Calculate the Word Error Rate (WER) between a reference and hypothesis sentence.\n    WER is defined as the minimum number of word-level edits (insertions, deletions, substitutions)\n    required to convert the hypothesis sentence into the reference sentence, divided by the total number of words in the reference.\n\n    Parameters:\n    reference (str): The ground truth sentence.\n    hypothesis (str): The predicted sentence.\n\n    Returns:\n    float: The word error rate between the reference and hypothesis.\n    \"\"\"\n\n    # Split the reference and hypothesis sentences into words\n    reference_words = reference.split()\n    hypothesis_words = hypothesis.split()\n\n    # Create a matrix to store the distances\n    d = np.zeros((len(reference_words) + 1, len(hypothesis_words) + 1), dtype=np.uint8)\n\n    # Initialize the matrix\n    for i in range(1, len(reference_words) + 1):\n        d[i][0] = i\n    for j in range(1, len(hypothesis_words) + 1):\n        d[0][j] = j\n\n    # Fill the matrix\n    for i in range(1, len(reference_words) + 1):\n        for j in range(1, len(hypothesis_words) + 1):\n            if reference_words[i - 1] == hypothesis_words[j - 1]:\n                d[i][j] = d[i - 1][j - 1]\n            else:\n                d[i][j] = min(d[i - 1][j], d[i][j - 1], d[i - 1][j - 1]) + 1\n\n    # The WER is the number of errors divided by the total number of words in the reference sentence\n    wer_value = d[len(reference_words)][len(hypothesis_words)] / len(reference_words)\n\n    return wer_value","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-17T20:36:59.694300Z","iopub.execute_input":"2024-10-17T20:36:59.694834Z","iopub.status.idle":"2024-10-17T20:36:59.705696Z","shell.execute_reply.started":"2024-10-17T20:36:59.694782Z","shell.execute_reply":"2024-10-17T20:36:59.704588Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"def avg_wer(ground_truths, predictions):\n    wer_loss=0\n    for reference, hypothesis in zip(ground_truths, predictions):\n        wer_loss += wer(reference, hypothesis)\n    return wer_loss/len(ground_truths)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-17T20:37:00.769216Z","iopub.execute_input":"2024-10-17T20:37:00.770000Z","iopub.status.idle":"2024-10-17T20:37:00.775452Z","shell.execute_reply.started":"2024-10-17T20:37:00.769955Z","shell.execute_reply":"2024-10-17T20:37:00.774150Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"model = model.to(device)\n\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n\nnum_epochs = 14","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-17T20:37:02.632185Z","iopub.execute_input":"2024-10-17T20:37:02.632934Z","iopub.status.idle":"2024-10-17T20:37:02.651013Z","shell.execute_reply.started":"2024-10-17T20:37:02.632890Z","shell.execute_reply":"2024-10-17T20:37:02.650051Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"# Define the learning rate scheduler based on validation WER loss\nLR_SCHEDULER = torch.optim.lr_scheduler.ReduceLROnPlateau(\n    optimizer, \n    mode='min',         # 'min' because we want to minimize the WER loss\n    factor=0.1,         # Reduce learning rate by a factor of 0.1\n    patience=1,         # Wait for 2 epochs without improvement before reducing LR\n    threshold=0.05,     # Minimum change in the monitored WER loss to qualify as improvement\n    verbose=True        # Print out messages when the learning rate is reduced\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-17T20:37:04.033126Z","iopub.execute_input":"2024-10-17T20:37:04.034072Z","iopub.status.idle":"2024-10-17T20:37:04.038954Z","shell.execute_reply.started":"2024-10-17T20:37:04.034024Z","shell.execute_reply":"2024-10-17T20:37:04.037984Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-17T20:37:05.567367Z","iopub.execute_input":"2024-10-17T20:37:05.567800Z","iopub.status.idle":"2024-10-17T20:37:05.572494Z","shell.execute_reply.started":"2024-10-17T20:37:05.567760Z","shell.execute_reply":"2024-10-17T20:37:05.571496Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"for epoch in range(num_epochs):\n    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n\n    # Set the model to training mode\n    model.train()\n\n    predictions = []\n    ground_truths = []\n\n    for batch in tqdm(train_loader, desc=\"Training Batches\"):\n        \n        processed_batch = preprocess_batch(batch)\n        \n        pixel_values = processed_batch['pixel_values'].to(device)\n        labels = processed_batch['labels'].to(device)\n\n        # Perform forward pass (compute logits and loss)\n        outputs = model(pixel_values=pixel_values, labels=labels)\n\n        # Get the predicted token IDs\n        predicted_ids = outputs.logits.argmax(dim=-1)\n\n        # Decode the predicted token IDs into strings\n        predicted_strings = tokenizer.batch_decode(predicted_ids, skip_special_tokens=True)\n\n        # Decode the ground truth labels into strings\n        true_ids = labels.detach().cpu().numpy()\n        true_strings = tokenizer.batch_decode(true_ids, skip_special_tokens=True)\n\n        # Collect the predictions and ground truths\n        predictions.extend(predicted_strings)\n        ground_truths.extend(true_strings)\n\n        # Compute loss and backpropagate\n        loss = outputs.loss\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n    # Calculate and print the Average Word Error Rate (WER) loss for training\n    avg_train_wer_loss = avg_wer(ground_truths, predictions)\n    print(f\"Average Training WER Loss: {avg_train_wer_loss:.4f}\")\n\n    # Update the learning rate scheduler\n    LR_SCHEDULER.step()\n\n    # Validation Phase\n    predictions = []\n    ground_truths = []\n\n    # Set the model to evaluation mode\n    model.eval()\n\n    with torch.no_grad():\n        for batch in tqdm(val_loader, desc=\"Validation Batches\"):\n            processed_batch = preprocess_batch(batch)\n\n            # Move pixel values and labels to the GPU\n            pixel_values = processed_batch['pixel_values'].to(device)\n            labels = processed_batch['labels'].to(device)\n\n            # Perform forward pass (compute logits, no backpropagation)\n            outputs = model(pixel_values=pixel_values, labels=labels)\n\n            # Get the predicted token IDs\n            predicted_ids = outputs.logits.argmax(dim=-1)\n\n            # Decode the predicted token IDs into strings\n            predicted_strings = tokenizer.batch_decode(predicted_ids, skip_special_tokens=True)\n\n            # Decode the ground truth labels into strings\n            true_ids = labels.detach().cpu().numpy()\n            true_strings = tokenizer.batch_decode(true_ids, skip_special_tokens=True)\n\n            # Collect the predictions and ground truths\n            predictions.extend(predicted_strings)\n            ground_truths.extend(true_strings)\n\n    # Calculate and print the Average WER loss for validation\n    avg_val_wer_loss = avg_wer(ground_truths, predictions)\n    print(f\"Average Validation WER Loss: {avg_val_wer_loss:.4f}\")\n\n    LR_SCHEDULER.step(avg_val_wer_loss)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-17T20:37:07.102641Z","iopub.execute_input":"2024-10-17T20:37:07.103041Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/14\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training Batches:   0%|          | 0/3202 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d416a72b1e4745d39fa7a997c9ae4b80"}},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"for i in range(len(predictions)):\n    print(f'Ground Truth:{ground_truths[i]}')\n    print(f'Preidction:{predictions[i]}')\n    print('='*30)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}